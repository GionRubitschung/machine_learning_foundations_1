{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wAF_GliiDrJ"
      },
      "source": [
        "# A First Jupyter Notebook to Explore\n",
        "\n",
        "## Machine Learning Example: A model that learns the similarity of words\n",
        "\n",
        "The first machine learning model we will examine in more detail belongs to the class of unsupervised models. We will cover this later this semester in class. For now our focus is on allowing you to get familiar with changing and running things on Jupyter Notebooks.\n",
        "\n",
        "Semantic spaces (also called word embeddings) are a form of machine learning models that tries to attempt to learn the relationships between words.\n",
        "\n",
        "What does this mean? While for a human it is simple to identify that speaking and spoken are very closely related and cat and dog share a relationship, this is not evident for a computer. How can we represent concepts in a form that allows computers to operate based on their meaning?\n",
        "\n",
        "Early attempts to do so (for example as part of search engines) were based on linguistic analysis of words. Identifying the common base form of verbs and plural and singular nouns allowed computer programs to \"understand\" that \"rocket\" and \"rockets\" are very closely related concepts.\n",
        "\n",
        "Modern approaches to represent the meaning of concepts are based on calculating a numeric representation (a vector with values) for concepts. Based on this approach it is possible to then to calculate how similar two concepts are by using a metric (e.g. a distance metric such as the euclidean distance or the cosine).\n",
        "Probably the most famous of these semantic word spaces is called Word2Vec ( [original Word2Vec implementation by Google](https://arxiv.org/pdf/1301.3781.pdf) ).\n",
        "The idea behind Word2Vec  is pretty simple. We are making and assumption that \"you can tell the meaning of a word by the company it keeps\" (Firth 1957 Linguist). This is analogous to the saying *show me your friends, and I'll tell who you are*. So if you have two words that have very similar neighbors (i.e. the usage context is about the same), then these words are probably quite similar in meaning or are at least highly related. For example, the words `shocked`,`appalled` and `astonished` are typically used in a similar context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UQ_W4N_iDrM"
      },
      "source": [
        "### Loading the program\n",
        "\n",
        "First, we start with importing the code we need to run our machine learning program. The code is shipped in so called `package`. Think of a `package` as a container or a zip file that contains all the code required to use it. At some point someone wrote the code for a machine learning algorithm that can learn the similarity of words and placed it in a `package` called `gensim`.\n",
        "It is an open source library developed by a machine learning consulting company from the Czech Republic. Among other algorithms it contains a reliable implementation of `Word2Vec`.\n",
        "\n",
        "If you are operating on a new environment or your own machine you will likely miss the `gensim` package and will have to install it. If we run the notebook on the Google Colab platform the module is already installed.\n",
        "In that case we just have to `import` the gensim package. Import means we load the module in the current environment so we can make use of it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JyCfYDYdiDrM"
      },
      "outputs": [],
      "source": [
        "# import needed modules\n",
        "import gzip\n",
        "import gensim\n",
        "import logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRq8fLiaiDrN"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "Apart from the implementation of the model, the second ingredient that is necessary for us to start with the training of unsupervised models is a dataset.\n",
        "\n",
        "You will find the following datasets on the course moodle page under `Introduction`.\n",
        "\n",
        "* `swiss-sms.txt.gz` is a set of Swiss German text messages (http://www.sms4science.ch/)\n",
        "* `reviews_data.txt.gz` is a collection of  English customer reviews from different web sites.\n",
        "* `reviews_data_small.txt.gz` is a smaller version of the above dataset for faster training times\n",
        "\n",
        "Upload the datasets into Colab using the sidebar on the left.\n",
        "In order to use Google Colab you will need a GMAIL account.\n",
        "Uploading the larger `reviews` dataset can take a couple of minutes.\n",
        "\n",
        "### Output a Line of the Dataset\n",
        "\n",
        "By executing the following `code cell` we print a line from from the dataset and get an idea what kind of text is contained. You can execute the cell multiple times. It will output a different random line every time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Jc2dyXjEiDrO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'Sep 27 2007\\tNEVER AGAIN! Horrible!\\tWe stayed only for one night and paid 150 pound! The room was in the basement - not very clean. The lady at the reception terrible. I had the feeling that she felt disturbed in reading her book. The breakfast was horrible. We then changed the hotel and got a much better one for 71 pound.THIS &quot;Athena Hotel&quot; is not a hotel! It is more or less a dosshouse (but a very expensive one)l Through your money out of the window and you have much jmore than staying at this place.NEVER AGAIN!!!!!!\\t'\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "data_file=\"./content/reviews_data.txt.gz\"\n",
        "\n",
        "with gzip.open (data_file, 'rb') as f:\n",
        "    lines = f.read().splitlines()\n",
        "    print(random.choice(lines))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEU6YiwtiDrO"
      },
      "source": [
        "### Read files into a list\n",
        "\n",
        "Now that we've had a sneak peak of our dataset, we can read it into a list so that we can pass this on to the `Word2Vec` machine learning algorithm for learning.\n",
        "\n",
        "The model learns based on the plain input text as we have printed it in the last cell. The only thing that happens before is some pre-processing when we make a call to `gensim.utils.simple_preprocess (line)`. This does some basic pre-processing such as tokenization (splitting the text into individual words), lowercasing, etc and returns back a list of tokens (words). Documentation of this pre-processing method can be found on the official [Gensim documentation site](https://radimrehurek.com/gensim/utils.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NEIVR2bDiDrP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reading file ./content/reviews_data.txt.gz...this may take a while\n",
            "read 0 lines\n",
            "read 10000 lines\n",
            "read 20000 lines\n",
            "read 30000 lines\n",
            "read 40000 lines\n",
            "read 50000 lines\n",
            "read 60000 lines\n",
            "read 70000 lines\n",
            "read 80000 lines\n",
            "read 90000 lines\n",
            "read 100000 lines\n",
            "read 110000 lines\n",
            "read 120000 lines\n",
            "read 130000 lines\n",
            "read 140000 lines\n",
            "read 150000 lines\n",
            "read 160000 lines\n",
            "read 170000 lines\n",
            "read 180000 lines\n",
            "read 190000 lines\n",
            "read 200000 lines\n",
            "read 210000 lines\n",
            "read 220000 lines\n",
            "read 230000 lines\n",
            "read 240000 lines\n",
            "read 250000 lines\n",
            "Done reading data file\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def read_input(input_file):\n",
        "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
        "\n",
        "    print(\"reading file {0}...this may take a while\".format(input_file))\n",
        "\n",
        "    with gzip.open (input_file, 'rb') as f:\n",
        "        for i, line in enumerate (f):\n",
        "\n",
        "            if (i%10000==0):\n",
        "                print(\"read {0} lines\".format (i))\n",
        "            # do some pre-processing and return a list of words for each review text\n",
        "            yield gensim.utils.simple_preprocess (line)\n",
        "\n",
        "# read the tokenized reviews into a list\n",
        "# each review item becomes a serries of words\n",
        "# so this becomes a list of lists\n",
        "documents = list (read_input (data_file))\n",
        "print(\"Done reading data file\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7Vsk3xKiDrP"
      },
      "source": [
        "## Training the Word2Vec model\n",
        "\n",
        "Training the model is fairly straightforward. You just instantiate Word2Vec (load the program into memory - then it is ready for execution) and pass the lines that we read in the previous step (we often call such inputs `documents` instead of using the term `line`). Word2Vec uses all these lines to internally create a vocabulary. And by vocabulary, I mean a set of unique words.\n",
        "\n",
        "After building the vocabulary, we just need to call `train(...)` to start training the Word2Vec model.\n",
        "Training time depends on the size of the training data. If you use the Reviews dataset training might take a couple of minutes on the Google Colab platform.\n",
        "The Swiss Text dataset is rather small and should train very quickly.\n",
        "\n",
        "Execute the code cell below in order to start the learning process of the Word2Vec algorithm. With the reviews data small dataset this will take approximately 5 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RqrwOGr_iDrQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done training the machine learning model.\n"
          ]
        }
      ],
      "source": [
        "model = gensim.models.Word2Vec (documents, vector_size=150, window=5, min_count=15, workers=10)\n",
        "model.train(documents,total_examples=len(documents),epochs=1)\n",
        "print(\"Done training the machine learning model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZQcQS7jiDrR"
      },
      "source": [
        "## Inspecting the Semantic Space\n",
        "\n",
        "Now that we have trained a first model we can start to inspect what the machine learning model has learnt.\n",
        "This first example shows a simple case of looking up the most similar words. All we need to do here is to call the `most_similar` function and provide a word as input. This returns the top 10 similar words.\n",
        "\n",
        "Execute the cell below to see the most similar words for the word defined in \"\".\n",
        "Test it with other words that come mind. If the word was not contained in the dataset that was used for the training then an error message will appear. We started with a blank model that knew nothing about human language, its knowledge about relations between words are limited to those that appeared in the dateset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "puYJPPFIiDrR"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('property', 0.8275459408760071),\n",
              " ('establishment', 0.6695693731307983),\n",
              " ('resort', 0.6585601568222046),\n",
              " ('place', 0.6144008636474609),\n",
              " ('accomodation', 0.5225114822387695),\n",
              " ('accommodation', 0.5152970552444458),\n",
              " ('motel', 0.5040884613990784),\n",
              " ('facility', 0.4804632365703583),\n",
              " ('travelodge', 0.47616931796073914),\n",
              " ('jbh', 0.46400994062423706)]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "w1 = \"hotel\"\n",
        "model.wv.most_similar (positive=w1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnzCeiGbiDrS"
      },
      "source": [
        "That looks pretty good, right? Let's look at a few more. As you can see below the `topn` parameter specifies the number of similar words to return."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6jlkQqN4iDrS"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('courteous', 0.9069100022315979),\n",
              " ('cordial', 0.8610674738883972),\n",
              " ('curtious', 0.8574371337890625),\n",
              " ('friendly', 0.8353148102760315),\n",
              " ('curteous', 0.8314024806022644),\n",
              " ('freindly', 0.8203408122062683)]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# look up top 6 words similar to 'polite'\n",
        "w1 = [\"polite\"]\n",
        "model.wv.most_similar (positive=w1,topn=6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktp1xfGtiDrS"
      },
      "source": [
        "That's, nice. You can even specify several positive examples to get things that are related in the provided context and provide negative examples to say what should not be considered as related. In the example below we are asking for all items that *relate to bed* only:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cYxhhccXiDrS"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('property', 0.759322464466095),\n",
              " ('establishment', 0.5897150635719299),\n",
              " ('jbh', 0.46248143911361694),\n",
              " ('motel', 0.4615221917629242),\n",
              " ('grandview', 0.4508134424686432),\n",
              " ('lrm', 0.43939098715782166),\n",
              " ('travelodge', 0.4390363097190857),\n",
              " ('hotell', 0.43639495968818665),\n",
              " ('hgvc', 0.43085941672325134),\n",
              " ('swissotel', 0.43083128333091736)]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get everything related to stuff on the bed\n",
        "w1 = ['hotel','resort']\n",
        "w2 = ['restaurant']\n",
        "model.wv.most_similar (positive=w1,negative=w2,topn=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgg4Gb-niDrT"
      },
      "source": [
        "### Similarity between two words in the vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPtPy9pHiDrT"
      },
      "source": [
        "You can even use the Word2Vec model to return the similarity between two words that are present in the vocabulary.\n",
        "This is the basic usage of the representations of the concepts.\n",
        "Having a numerical representation allows us to compute the similarity between any two words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZDpqFseHiDrT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.21008974"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# similarity between two different words\n",
        "model.wv.similarity(w1=\"bed\",w2=\"sleep\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CrTzX53FiDrT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.25024793"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# similarity between two identical words\n",
        "model.wv.similarity(w1=\"hotel\",w2=\"restaurant\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VesJkzoAiDrU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.13489385"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# similarity between two unrelated words\n",
        "model.wv.similarity(w1=\"hotel\",w2=\"street\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B_djCggiDrU"
      },
      "source": [
        "Under the hood, the above three snippets computes the cosine similarity between the two specified words using word vectors of each. If you do a similarity between two identical words, the score will be 1.0 as the range of the cosine similarity score will always be between [0.0-1.0]. You can read more about cosine similarity scoring [here](https://en.wikipedia.org/wiki/Cosine_similarity)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO47sWcLiDrU"
      },
      "source": [
        "### Find the odd one out\n",
        "You can even use Word2Vec to find odd items given a list of items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gcvpUUw5iDrU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'street'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Which one is the odd one out in this list?\n",
        "model.wv.doesnt_match([\"hotel\",\"motel\",\"street\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVkHiIbWiDrU"
      },
      "source": [
        "## Understanding some of the (hyper-) parameters\n",
        "To train the model earlier, we had to set some hyperparameters. Now, let's try to understand what some of them mean. For reference, this is the command that we used to train the model.\n",
        "\n",
        "```\n",
        "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)\n",
        "```\n",
        "\n",
        "### `size`\n",
        "The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. 100-150 dimensions are common dimensions.\n",
        "\n",
        "### `window`\n",
        "The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give allow you to strengthen relationships that are synonymous, while larger window sizes favor associative relationships.\n",
        "\n",
        "    The distinction between synonymous and associative relationships is based on findings in cognitive linguistics. Based on word priming experimentation, two main relations between words have been identified (see [CHIA1990]): synonymous relations (also referred to as similar or semantic relations in the cognitive science literature) and associative relations. As outlined in [CHIA1990], the distinction for both relationship types is not exclusive; that is, word relations are not exclusively synonymous or associative. Doctor - Nurse is an example of a word relation that can be considered as being of a synonymous-associative nature.\n",
        "\n",
        "\n",
        "    Two terms/words are associatively related if they often appear in a shared context. The following are examples of this type of relationship:\n",
        "\n",
        "            Spider - Web\n",
        "            Google - Page rank\n",
        "            Smoke - Cigarette\n",
        "            Phone - Call\n",
        "            Lighter - Fire\n",
        "\n",
        "    Two terms/words are synonymously related if they share common features. The following are examples of this type of relationship:\n",
        "\n",
        "            Wolf - Dog\n",
        "            Cetacean - Whale\n",
        "            Astronaut - Man\n",
        "            Car - Van\n",
        "            Smartphone - iPhone 4s\n",
        "\n",
        "[CHIA1990]\t(1, 2) Chiarello, Christine, et al. Semantic and associative priming in the cerebral hemispheres: Some words do, some words don’t... sometimes, some places. Brain and language 38.1 (1990): 75-104\n",
        "\n",
        "\n",
        "### `min_count`\n",
        "Minimium frequency count of words. The model would ignore words that do not statisfy the `min_count`. Extremely infrequent words are usually unimportant (spelling mistakes, non-words), so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n",
        "\n",
        "### `workers`\n",
        "How many threads to use behind the scenes?\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
