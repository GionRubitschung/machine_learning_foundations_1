{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6686D6; font-family: 'Helvetica Neue', sans-serif;\"><small>L7S2N1</small> Optimise Me</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #6686D6; font-family: 'Helvetica Neue', sans-serif;\">1. Initial Setup</h3>\n",
    "\n",
    "Below is the initial set up to re-instate our stack for pre-processing and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tweets_df = pd.read_csv(\"./Brexit-Non-Brexit-100K.csv\", delimiter=\";\", encoding='utf-8')\n",
    "# We have to do some minimal clean-up of the dataset and replace missing values with empty strings (an empty string is still a string)\n",
    "# If we don't do this we will run into an exception when we use the CountVectoriser\n",
    "tweets_df['tweet'] = tweets_df['tweet'].replace(np.nan, '', regex=True)\n",
    "\n",
    "#\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets_df['tweet'], tweets_df['label'], test_size=0.2, random_state=42)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# Create a text processing and classification pipeline\n",
    "ml_pipeline = make_pipeline(CountVectorizer(min_df=0.001), MultinomialNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Baseline Documentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9582\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "ml_pipeline.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, ml_pipeline.predict(X_test) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #6686D6; font-family: 'Helvetica Neue', sans-serif;\">2. Two Optimisation Vectors</h3>\n",
    "\n",
    "The above accuracy already represents a rather strong baseline.\n",
    "We know want to explore how we can improve further by focusing on two focus points:\n",
    "* provisioning process of the data\n",
    "* ML training process.\n",
    "\n",
    "For the following exercises please form groups of 2-3 people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Data Curation & Provisioning Optimisation\n",
    "\n",
    "We will for the moment exclude a commmon source for optimisation: the expansion of our dataset.\n",
    "Adding data is often an effective way to improve the overall performance.\n",
    "However in this case, when we focus on Twitter data, it can be quite tedious and labor intensive to collect additional data. \n",
    "\n",
    "Instead we will focus on the transformation and pre-processing step of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Exercise: Pre-processing and Vectorisation\n",
    "\n",
    "Try to improve the performance by making changes to the vectorisation step we have used.\n",
    "Read up on the options under: [Text feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97485\n"
     ]
    }
   ],
   "source": [
    "# Use this cell to re-train\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "ml_pipeline = make_pipeline(CountVectorizer(min_df=0.001), LogisticRegression(max_iter=1000))\n",
    "ml_pipeline.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, ml_pipeline.predict(X_test) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Algorithm Optimisation\n",
    "\n",
    "This is usually the most commonly targeted step with respect to optimisation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Exercise: Optimise ML Algorithm\n",
    "\n",
    "Visit the following documentation site of sci-kit learn and identify ML algorithms that could be used to train for our classification task.\n",
    "\n",
    "[Listing of supervised ML algorithms](https://scikit-learn.org/stable/supervised_learning.html)\n",
    "\n",
    "Use the cell below to train and optimise on the algorithm side.\n",
    "If you achieved an improvement in 2.2. then please take over the positive changes you made for the vectorisation step.\n",
    "Consider changing the algorithm class as well as the hyperparameters of the algorithm you use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9582\n"
     ]
    }
   ],
   "source": [
    "ml_pipeline = make_pipeline(CountVectorizer(min_df=0.001), MultinomialNB())\n",
    "ml_pipeline.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, ml_pipeline.predict(X_test) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #6686D6; font-family: 'Helvetica Neue', sans-serif;\">3. Automating Optimisation</h3>\n",
    "\n",
    "One observation you should be able to make is that the amount of combinations we encounter in optimisation set ups is quite large.\n",
    "This is sometimes called the parameter space.\n",
    "\n",
    "In order to support us in finding the optimal setup for the combinations we can utilize automation for the execution of our training runs. \n",
    "\n",
    "Use the code sample below to define training runs that further optimise on your best setup you have achieved so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Parameters:  {'vect__max_df': 0.3, 'vect__min_df': 0.001}\n",
      "Best Score:  0.9765866585411589\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'vect__max_df': (0.3, 0.4, 0.5, 0.6),\n",
    "    'vect__min_df': (0.001, 0.002, 0.003),\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Perform the grid search on the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Cross-Validation\n",
    "\n",
    "The above code uses `GridSearch**CV**`. The `CV` part stands for cross-validation. Cross-validation is a more sophisticated approach to our test and train splitting. \n",
    "It has the same motivation as splitting the data into a test and train portion. Instead of splitting once, Cross-Validation splits N times. \n",
    "So if we talk about N-Fold Cross-Validation the N refers to the times we split the data randomly. 10-fold cross-validation means we create ten random splits of our data and measure the performance for each of these train-test setups.\n",
    "\n",
    "1. **Random Sampling:** In cross-validation, the dataset is randomly divided into a fixed number of parts or \"folds.\" This random division ensures that each fold is a representative sample of the overall dataset, covering various aspects of the data's variability.\n",
    "\n",
    "2. **Sequential Evaluation:** The model is trained and tested multiple times, each time with a different fold acting as the test set and the remaining folds combined to form the training set. This sequential process ensures that every data point is used for both training and testing across the iterations.\n",
    "\n",
    "3. **Independent Testing:** In each iteration, the model is tested on data that it hasn't seen during training. This is crucial for evaluating the model's performance on new, unseen data, mimicking real-world situations where the model will encounter data variations.\n",
    "\n",
    "4. **Aggregated Results:** After all iterations, the performance metrics (like accuracy, precision, etc.) from each fold are aggregated. This aggregated result is a more reliable measure of the model's performance than a single train-test split, as it accounts for the variability in the dataset.\n",
    "\n",
    "5. **Mitigating Overfitting:** Cross-validation helps in detecting overfitting. Overfitting occurs when a model performs exceptionally well on the training data but poorly on new data. By using multiple random samples, cross-validation exposes the model to various data scenarios, ensuring that the model's performance is consistent across different data samples.\n",
    "\n",
    "6. **Enhanced Reliability:** The use of random samples in cross-validation ensures that the model's evaluation is not biased by any particular arrangement or peculiarity of the data. This enhances the reliability of the evaluation, making the model more trustworthy for practical applications.\n",
    "\n",
    "In summary, cross-validation with random sampling is a robust method for assessing the generalizability and effectiveness of a machine learning model. It ensures that the model is tested under various scenarios, reflecting its likely performance in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Discussion & Outlook\n",
    "\n",
    "* Large Parameter Spaces\n",
    "* Effectiveness of Automation\n",
    "* Auto-ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #6686D6; font-family: 'Helvetica Neue', sans-serif;\">4. Model Documentation</h3>\n",
    "\n",
    "#### 4.1 Exercise: Best Runs\n",
    "\n",
    "Navigate to the following Microsoft list and start documenting your runs.\n",
    "\n",
    "* [Run documentation](https://bernerfachhochschule.sharepoint.com/:l:/s/ti-bscdataengineering/FHp4B79xst1HjZfgPQHlnt4BRGeVdXH10vrHsrmSkZ8j0Q?e=zia76E) \n",
    "\n",
    "#### 4.2 Discussion: Performance only a Part of the Story\n",
    "\n",
    "In this notebook we focused on improving the accuracy of the model.\n",
    "For real-world application of our models there are other aspects that are very important:\n",
    "\n",
    "\n",
    "* **Overfitting and Underfitting:** Ensuring that the model generalizes well to new, unseen data, rather than memorizing the training data (overfitting) or being too simple to capture the patterns in the data (underfitting).\n",
    "\n",
    "* **Model Complexity and Efficiency:** The trade-off between the model's complexity and its performance. More complex models might perform better but can require more data and computational resources.\n",
    "\n",
    "* **Real-World Application Fit:** The quality of the training data, and whether the data is representative of the real-world scenarios where the model will be applied. Biased data can lead to biased predictions.\n",
    "\n",
    "* **Robustness and Stability:** The model's ability to perform consistently across different datasets and in the presence of noisy or imperfect data.\n",
    "\n",
    "* **Interpretability and Explainability:** Understanding why the model makes certain predictions, which is critical in many applications, especially those that require trust and transparency.\n",
    "\n",
    "* **Ethical Considerations:** Ensuring that the model does not perpetuate or exacerbate unfair biases, and is ethically sound in its application.\n",
    "\n",
    "* **Cost of Errors:** The real-world impact of errors made by the model, which can vary significantly depending on the application.\n",
    "\n",
    "* **Scalability:** How well the model can be scaled to handle larger datasets or be deployed in different environments.\n",
    "\n",
    "* **Regulatory Compliance:** Ensuring that the model complies with relevant laws and regulations, particularly in industries like finance and healthcare.\n",
    "\n",
    "* **Environmental Impact:** The energy consumption and environmental impact of training and deploying the model, especially for large, complex models.\n",
    "\n",
    "Each of these considerations plays a crucial role in the overall evaluation and deployment of machine learning models, and the importance of each can vary depending on the specific application and context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
