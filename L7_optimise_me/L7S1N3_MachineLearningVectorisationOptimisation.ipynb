{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60645ad6",
   "metadata": {},
   "source": [
    "\n",
    "# Machine Learning Vectorisation & Optimisation\n",
    "\n",
    "This exercise focused notebook builds up on the concepts introduced in L6S4N2.\n",
    "\n",
    "We shift our focus on the **bold highlighted** steps steps of the machine learning workflow.\n",
    "\n",
    "1. Dataset Curation\n",
    "2. **Dataset Provisioning**\n",
    "3. Model Training Run\n",
    "4. Evaluation\n",
    "5. **Iterative Optimisation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf9b63e",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Dataset Curation\n",
    "\n",
    "The data we will use are tweets collected from the UK around the time-frame of the original Brexit discussion.\n",
    "Please note that the data is not filtered in any way and might contain offensive content. \n",
    "\n",
    "The data has been annotated with two classes:\n",
    "* Brexit : tweets that relate to the topic Brexit\n",
    "* non-Brexit : tweets about other topics\n",
    "\n",
    "Contrary to the previous notebook the dataset features a more balanced distribution between the classes and consists of 1000k tweets.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40aac43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns of the dataframe are: Index(['tweet', 'label'], dtype='object').\n",
      "The shape of the dataframe is: (99997, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "tweets_df = pd.read_csv(\"./Brexit-Non-Brexit-100K.csv\", delimiter=\";\", encoding='utf-8')\n",
    "\n",
    "# We have to do some minimal clean-up of the dataset and replace missing values with empty strings (an empty string is still a string)\n",
    "# If we don't do this we will run into an exception when we use the CountVectoriser\n",
    "tweets_df['tweet'] = tweets_df['tweet'].replace(np.nan, '', regex=True)\n",
    "\n",
    "\n",
    "print(f\"The columns of the dataframe are: {tweets_df.columns}.\")\n",
    "print(f\"The shape of the dataframe is: {tweets_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e968fef8",
   "metadata": {},
   "source": [
    "### 1.1. Exercise: Sanity Check & Getting Familiar with the Data\n",
    "\n",
    "As seasoned data engineers you are aware that at a minimum you should verify that the data has loaded correctly via `read_csv()`.\n",
    "Take a look at the dataframe. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6228daed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Brexit</th>\n",
       "      <td>26579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>non-Brexit</th>\n",
       "      <td>73418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tweet\n",
       "label            \n",
       "Brexit      26579\n",
       "non-Brexit  73418"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203fe0d4",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Dataset Provisioning\n",
    "\n",
    "We'll split our dataset into a training set and a testing set as was done before. The training set is used to train the model, and the testing set is used to evaluate its performance.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb7bc9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets_df['tweet'], tweets_df['label'], test_size=0.2, random_state=42)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db17b9ad",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Model Training\n",
    "\n",
    "\n",
    "### 3.1 Create a Machine Learning Pipeline\n",
    "\n",
    "We utilize the same pipeline that first converts the text data into a format suitable for machine learning (using `CountVectorizer`), and then applies a classification algorithm (in this case, `MultinomialNB`).\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0aa021fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Create a text processing and classification pipeline\n",
    "# Note that this has named steps so that we can access the individual parts of the pipeline.\n",
    "ml_pipeline = Pipeline([\n",
    "    ('countVectorizer',CountVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22120645",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2 Train the Model\n",
    "\n",
    "With the pipeline set up, we can now train our model on the training data.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac80db1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;countVectorizer&#x27;, CountVectorizer()),\n",
       "                (&#x27;classifier&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;countVectorizer&#x27;, CountVectorizer()),\n",
       "                (&#x27;classifier&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('countVectorizer', CountVectorizer()),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "ml_pipeline.fit(X_train, y_train)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1081db25",
   "metadata": {},
   "source": [
    "Two things happen when we execute the pipeline:\n",
    "\n",
    "1. **The CountVectorizer is fitted.**\n",
    "    * It cleans up all the text samples as described above\n",
    "    * It then identifies all unique terms that appear in the input dataset\n",
    "    * It stores those terms in a dictionary\n",
    "    * Finally it transforms the tweets into a vectorised form\n",
    "2. **The model is fitted (trained)**\n",
    "    * The vectorised tweets (by convention we call this `X`) and the labels (by convention we call this `y`) are passed to the ML model\n",
    "    * The ML model is trained \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2370e7e4",
   "metadata": {},
   "source": [
    "### 3.3 Exercise: Exploring and Understanding the `CountVectorizer`\n",
    "\n",
    "Explore the `CountVectorizer`.\n",
    "\n",
    "Visit the documentation of the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) \n",
    "\n",
    "\n",
    "**a) Accessing the elements of the pipeline**\n",
    "\n",
    "You can access the elements of the pipeline in the following way:\n",
    "\n",
    "```python\n",
    "ml_pipeline.named_steps['countVectorizer']\n",
    "```\n",
    "\n",
    "**b) Check the size of the vocabulary** \n",
    "\n",
    "There is an attribute `vocabulary_` that you can use to access the fitted vocabulary (all unique terms found in the tweets) of the `CountVectoriser`.\n",
    "Have a guess how many unique terms you expect then check the actual size of the vocabulary. \n",
    "\n",
    "**c) Check what is contained in the vocabulary**\n",
    "\n",
    "There are multiple good reasons for checking the content of the vocabulary.\n",
    "This does not only apply to the use of the `CountVectoriser` in this example, but is generally true for all the pre-processing and transformation that is applied to any data that touches your `ML` or `Analytics` workflow. \n",
    "\n",
    "<div style=\"background-color: #e8f4f8; border-left: 5px solid #6fa8dc; padding: 10px; margin: 10px 0; font-size: 1em; line-height: 1.4;\">\n",
    "    <p><b>Practical Relevance:</b>\n",
    "    <p>Familiarizing yourself with the data and its representation as features is extremely important. It serves the following purpose:\n",
    "        <ul>\n",
    "            <li>Identify data transformation errors: Data transformation and loading is subject to all kinds of errors. Starting with \n",
    "            using a wrong `delimiter`, to the application of a wrong `encoding`, to mistakes based on wrong `escaping` assumptions (Don't worry if you are not yet familiar with all these terms, we will dive into this in the Data Engineering Fundamentals 1 course.). Many of these mistakes can be visually identified.\n",
    "            </li>\n",
    "            <li>Getting to know your features: It is important that you develop an intuition for the features that your models are fed. In this case each unique term is a feature as we are working with text and apply the vectorisation defined by the CountVectoriser. Knowing what our features will look like gives us ideas how we can improve the features, and what might be advantages and limitations of the feature representation we chose.</li>\n",
    "        </ul>\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "Assume we have a list named `vocabulary`:\n",
    "\n",
    "We have the following options available to analyse the contents.\n",
    "\n",
    "### 1. Storing in a CSV File Using Pandas\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Convert list to DataFrame\n",
    "df = pd.DataFrame(vocabulary, columns=['vocabulary'])\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('vocabulary.csv', index=False)\n",
    "```\n",
    "\n",
    "### 2. Printing Using `pprint`\n",
    "\n",
    "```python\n",
    "from pprint import pprint\n",
    "\n",
    "# Pretty-print the list\n",
    "pprint(vocabulary)\n",
    "```\n",
    "\n",
    "### 3. Printing by Placing in a Pandas DataFrame\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Convert list to DataFrame\n",
    "df = pd.DataFrame(vocabulary, columns=['vocabulary'])\n",
    "\n",
    "# Print DataFrame\n",
    "print(df)\n",
    "```\n",
    "\n",
    "In each of these methods, the list `vocabulary` is represented in different forms: as a CSV file, pretty-printed list, and a DataFrame, providing flexibility in how the data can be stored and displayed.\n",
    "\n",
    "\n",
    "Use the code cell below to explore the different approaches and check the content of the vocabulary. **Note:** you might have to transform what you get from the `CountVectoriser` in order to make it work. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "469b86f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249101\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(len(ml_pipeline.named_steps['countVectorizer'].vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ae5b3e",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Evaluation\n",
    "\n",
    "After training the model, we use it to make predictions on the test dataset.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19e19b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict labels for the test set\n",
    "predictions = ml_pipeline.predict(X_test)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3efb93b",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1 Measure Precision\n",
    "Finally, we evaluate the model's performance by looking at its accuracy and a detailed classification report.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d39b5b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.58765\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adce8d68",
   "metadata": {},
   "source": [
    "### 4.2 Analysing Measurements\n",
    "\n",
    "What do you think about the measured accuracy?\n",
    "Are you satisfied with the model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f421a6af",
   "metadata": {},
   "source": [
    "### 4.3 Exercise: Digging into the Metrics\n",
    "\n",
    "Scikit-learn provides several methods for analyzing the predictions of a model. Some of these methods include:\n",
    "\n",
    "1. **[Confusion Matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)**\n",
    "\n",
    "2. **[Classification Report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)**\n",
    "\n",
    "As in the last notebook use these methods to analyse the performance and behavior of the model in more detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bdafa8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Brexit       0.39      0.98      0.56      5373\n",
      "  non-Brexit       0.98      0.44      0.61     14627\n",
      "\n",
      "    accuracy                           0.59     20000\n",
      "   macro avg       0.69      0.71      0.59     20000\n",
      "weighted avg       0.83      0.59      0.60     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    " \n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989bdc55",
   "metadata": {},
   "source": [
    "# 5. Efficiently Optimizing Machine Learning Models\n",
    "\n",
    "## 5.1. Define a Baseline Model\n",
    "### Purpose\n",
    "- Establishing a baseline is essential to set a reference point to compare the performance of future, more complex models.\n",
    "- It provides a minimum performance threshold and is typically simple and easy to understand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64659af0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8f78158",
   "metadata": {},
   "source": [
    "### 5.1 Exercise: Baseline Model Documentation\n",
    "\n",
    "Document the baseline model in the markdown cell below.\n",
    "Think about the things that should be documented.\n",
    "\n",
    "Remember this serves two purposes:\n",
    "* Provide you with a basis for your optimisations\n",
    "* Document and save your work in order to allow the next team member or you to repeatably train the model or extend it at a later stage.\n",
    "\n",
    "Discuss with your lecturer the practical relevance of this.\n",
    "\n",
    "Use the cell below to write your documentation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ceb650",
   "metadata": {},
   "source": [
    "**Baseline Documentation**\n",
    "\n",
    "Initial Pipeline:\n",
    "\n",
    "- ml_pipeline = Pipeline([\n",
    "    ('countVectorizer',CountVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "- Vocabulary size: 249101\n",
    "- Accuracy: 0.58765\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ae7cc",
   "metadata": {},
   "source": [
    "## 5.2. Algorithm Optimisation\n",
    "\n",
    "Look at the curent model we employed in the pipeline.\n",
    "\n",
    "### 5.2 Exercise: Hyperparameter Optimisation\n",
    "\n",
    "Try to identify if the current training can be optimised by trying different hyperparameters.\n",
    "Take note of your training success. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a7decc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.9639 0.11 455'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and evaluate in this cell\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "ml_pipeline_2 = Pipeline([\n",
    "    ('countVectorizer',CountVectorizer(max_df=0.16, min_df=456)),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "ml_pipeline_2.fit(X_train, y_train)\n",
    "predictions = ml_pipeline_2.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "\"0.8686 0.8, 0.05\"\n",
    "\"0.8861 1.0 2\"\n",
    "\"0.91845 1.0 3\"\n",
    "\"0.92775 1.0 4\"\n",
    "\"0.9331 1.0 5\"\n",
    "\"0.93655 1.0 6\"\n",
    "\"0.9399 1.0 7\"\n",
    "\"0.94155 1.0 8\"\n",
    "\"0.95645 1.0 30\"\n",
    "\"0.9583 1.0 50\"\n",
    "\"0.95875 1.0 60\"\n",
    "\"0.96145 1.0 200\"\n",
    "\"0.9627 1.0 300\"\n",
    "\"0.96295 1.0 400\"\n",
    "\"0.96355 1.0 450\"\n",
    "\"0.9636 1.0 455\"\n",
    "\"0.9639 0.11 455\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6337b81c",
   "metadata": {},
   "source": [
    "**Document your training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0bb7f3",
   "metadata": {},
   "source": [
    "### 5.2 Exercise: Algorithm Optimisation\n",
    "\n",
    "Identify other suitable algorithms (classification algorithms) that are available on sci-kit learn (only those).\n",
    "Train and try to optimise your performance by swapping out the algorithm used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "702ec113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97695\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate in this cell\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "ml_pipeline_2 = Pipeline([\n",
    "    ('countVectorizer',TfidfVectorizer(max_df=0.15, min_df=10)),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "ml_pipeline_2.fit(X_train, y_train)\n",
    "predictions = ml_pipeline_2.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53042300",
   "metadata": {},
   "source": [
    "**Document your training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9c1798",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
